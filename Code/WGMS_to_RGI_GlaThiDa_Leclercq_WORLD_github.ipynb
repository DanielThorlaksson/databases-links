{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import numpy as np\n",
    "import shapely.geometry as shpg\n",
    "import matplotlib.pyplot as plt\n",
    "import salem\n",
    "from glob import glob\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from salem import datasets\n",
    "from cleo import Map\n",
    "import shapely.ops\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wgms_dir = '..\\\\DOI-WGMS-FoG-2014-09'\n",
    "rgi_dir_sup = '..\\\\rgi50'\n",
    "output_dir = '..\\\\WGMS_LINKS'\n",
    "f_A = os.path.join(wgms_dir, 'WGMS-FoG-2014-09-A-GENERAL-INFORMATION.csv')\n",
    "f_EE = os.path.join(wgms_dir, 'WGMS-FoG-2014-09-EE-MASS-BALANCE.csv')\n",
    "r_rgi = os.path.join(rgi_dir_sup, '00_rgi50_regions\\\\00_rgi50_O1Regions.shp')\n",
    "f_rgi_links = '..\\\\00_rgi50_links.csv'\n",
    "f_gla = '..\\\\Glathida_2014\\\\T.csv'\n",
    "f_lec = '..\\\\Leclercq\\\\glacier_properties.txt'\n",
    "\n",
    "# in order not to treat the glaciers again that we have already linked\n",
    "f_links = '..\\\\WGMS_LINKS\\\\Manual_links_WGMS_to_RGI_GlaThiDa_Leclercq_ALPS_20160221.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Haversine function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between one point \n",
    "    on the earth and an array of points (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a)) \n",
    "    r = 6371000 # Radius of earth in meters\n",
    "    return c * r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read RGI - only regions this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rgi_regions = gpd.read_file(r_rgi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read also the GlaThiDa link file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gla = pd.read_csv(f_gla, header=0, encoding='iso8859_15', sep=';', low_memory=False)\n",
    "gla.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read also the Leclercq link file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lec = pd.read_csv(f_lec, sep='\\t', na_values='-99999')\n",
    "lec = lec.rename(columns=lambda x: x.strip())\n",
    "lec.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read also the file with already linked glaciers in the Alps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "safe = pd.read_csv(f_links)\n",
    "safe_ids = safe.WGMS_ID.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select WGMS glaciers with > 5 MB measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pda = pd.read_csv(f_A, encoding='iso8859_15')\n",
    "pdee = pd.read_csv(f_EE, encoding='iso8859_15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_ids = pda.WGMS_ID.values\n",
    "pdee = pdee[pdee.WGMS_ID.isin(all_ids)]\n",
    "# Where we have yearly measures and no altitude range\n",
    "pdee = pdee[pdee.LOWER_BOUND.isin([9999])]\n",
    "gp_id = pdee.groupby('WGMS_ID')\n",
    "ids_5 = []\n",
    "for wgmsid, group in gp_id:\n",
    "    if np.sum(np.isfinite(group.ANNUAL_BALANCE.values)) >= 5:\n",
    "        ids_5.append(wgmsid)\n",
    "print len(ids_5)\n",
    "\n",
    "# make them th selected IDs\n",
    "ids_sel = ids_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exclude those glaciers that are already linked to RGI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if rgi links are contained totally in the selected glaciers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rgi_links = pd.read_csv(f_rgi_links, skiprows=[0,1])\n",
    "\n",
    "not_cont_ids = set(rgi_links.FoGId.values) - set(ids_sel)\n",
    "print len(not_cont_ids)\n",
    "subset =  rgi_links[rgi_links.FoGId.isin(not_cont_ids)][['Name', 'FoGId', 'NYears']]\n",
    "subset = subset[subset.NYears >= 5]\n",
    "print subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, the RGI links are not totally contained in the selected glaciers, because MOST OF THOSE NOT CONTAINED have less than 5 MB measurements.\n",
    "Exceptions are Langjoekull Southern Dome (13MB) and SU5A03003026 Obruchev Glacier (24MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print pda[pda.WGMS_ID == 729]\n",
    "for wgmsid, group in gp_id:\n",
    "    if wgmsid == 729:\n",
    "        print np.sum(np.isfinite(group.ANNUAL_BALANCE.values))\n",
    "        \n",
    "print pda[pda.WGMS_ID == 3101]\n",
    "for wgmsid, group in gp_id:\n",
    "    if wgmsid == 729:\n",
    "        print np.sum(np.isfinite(group.ANNUAL_BALANCE.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay: IF the value for 'NYears' is the number of MB measurements, they simply do only have 4 measurements each in the pdee file. SO we still can do the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtract already linked RGI glaciers from selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ids_sel = set(ids_sel) - set(rgi_links.FoGId.values)\n",
    "print len(ids_sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for each of the RGI regions which WGMS glaciers are contained and produce plots, accordingly (not all RGI must be in memory at same time then)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign first RGI region to WGMS glaciers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ct = 0\n",
    "pda_sel = pda[pda.WGMS_ID.isin(ids_sel)]\n",
    "pda_sel['rgi_region'] = ''\n",
    "print len(pda_sel)\n",
    "for index, row in pda_sel.iterrows():\n",
    "    for poly in rgi_regions.geometry.values:\n",
    "        if shpg.Point(row['LONGITUDE'], row['LATITUDE']).within(poly):\n",
    "            pda_sel.rgi_region[pda_sel.index == index] = rgi_regions[rgi_regions.geometry == poly].Primary_ID.iloc[0]\n",
    "            ct+=1\n",
    "            if ct % 10 ==0:\n",
    "                print ct\n",
    "            break\n",
    "print 'done.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pda_sel.to_csv(os.path.join(output_dir, 'WGMS_RGI_regions_table.csv'), encoding='iso8859_15')\n",
    "pda_sel.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A dictionary that links name of the region to RGI file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reg_dict = {'Alaska':'01_rgi50_Alaska\\\\01_rgi50_Alaska.shp',\n",
    "            'Antarctic and Subantarctic':'19_rgi50_AntarcticSubantarctic\\\\19_rgi50_AntarcticSubantarctic.shp',\n",
    "            'Arctic Canada North':'03_rgi50_ArcticCanadaNorth\\\\03_rgi50_ArcticCanadaNorth.shp', \n",
    "            'Arctic Canada South':'04_rgi50_ArcticCanadaSouth\\\\04_rgi50_ArcticCanadaSouth.shp',\n",
    "            'Caucasus and Middle East':'12_rgi50_CaucasusMiddleEast\\\\12_rgi50_CaucasusMiddleEast.shp',\n",
    "            'Central Asia':'13_rgi50_CentralAsia\\\\13_rgi50_CentralAsia.shp',\n",
    "            'Central Europe':'11_rgi50_CentralEurope\\\\11_rgi50_CentralEurope.shp',\n",
    "            'Greenland':'05_rgi50_GreenlandPeriphery\\\\05_rgi50_GreenlandPeriphery.shp',\n",
    "            'Iceland':'06_rgi50_Iceland\\\\06_rgi50_Iceland.shp',\n",
    "            'Low Latitudes':'16_rgi50_LowLatitudes\\\\16_rgi50_LowLatitudes.shp',\n",
    "            'New Zealand':'18_rgi50_NewZealand\\\\18_rgi50_NewZealand.shp',\n",
    "            'North Asia':'10_rgi50_NorthAsia\\\\10_rgi50_NorthAsia.shp',\n",
    "            'Russian Arctic':'09_rgi50_RussianArctic\\\\09_rgi50_RussianArctic.shp',\n",
    "            'Scandinavia':'08_rgi50_Scandinavia\\\\08_rgi50_Scandinavia.shp',\n",
    "            'South Asia East':'15_rgi50_SouthAsiaEast\\\\15_rgi50_SouthAsiaEast.shp',\n",
    "            'South Asia West':'14_rgi50_SouthAsiaWest\\\\14_rgi50_SouthAsiaWest.shp', \n",
    "            'Southern Andes':'17_rgi50_SouthernAndes\\\\17_rgi50_SouthernAndes.shp',\n",
    "            'Svalbard':'07_rgi50_Svalbard\\\\07_rgi50_Svalbard.shp',\n",
    "            'Western Canada and US':'02_rgi50_WesternCanadaUS\\\\02_rgi50_WesternCanadaUS.shp'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One manual extension necessary (glacier not in a single region polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pda_sel.rgi_region[pda_sel.NAME == 'HAMAGURI YUKI'] = 'North Asia'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a pandas DataFrame for the result links - basically a copy of the pda frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = pda_sel[['POLITICAL_UNIT', 'NAME', 'WGMS_ID', 'LATITUDE', 'LONGITUDE']]\n",
    "results['RGI_ID'] = np.nan\n",
    "results['GlaThiDa_ID'] = np.nan\n",
    "results['Leclercq_ID'] = np.nan\n",
    "results['remark'] = np.nan\n",
    "lec.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with PdfPages(os.path.join(output_dir, 'WGMS_Glaciers_WORLD_ALL.pdf' )) as pdf:\n",
    "    for regio in np.unique(pda_sel.rgi_region.values):\n",
    "        print regio\n",
    "        pda_sel_regio = pda_sel[pda_sel.rgi_region == regio]\n",
    "\n",
    "        pdrgi = gpd.read_file(os.path.join(rgi_dir_sup,reg_dict[regio]), encoding='iso8859_15')\n",
    "\n",
    "        curr = 1  # current plot\n",
    "        total = len(pda_sel_regio)  # total number of plots ()\n",
    "\n",
    "        for gid in pda_sel_regio.WGMS_ID.values:\n",
    "            glacier = pda_sel_regio[pda_sel_regio.WGMS_ID == gid].iloc[0]\n",
    "            print 'Glacier: ', glacier.NAME, ' ', glacier.GEN_LOCATION, ' ', glacier.SPEC_LOCATION\n",
    "            lon, lat = glacier.LONGITUDE, glacier.LATITUDE\n",
    "            pdrgi['DIST'] = haversine(lon, lat, pdrgi.CenLon.values, pdrgi.CenLat.values)\n",
    "            sortrgi = pdrgi.sort(columns='DIST')\n",
    "\n",
    "            # For GoogleMap we need a lon lat range to generate the map\n",
    "            mmlon = [lon, lon]\n",
    "            mmlat = [lat, lat]\n",
    "\n",
    "            ###################################################################################################\n",
    "            # For the case the glacier is already checked manually (in the Alps)\n",
    "            if regio == 'Central Europe' and (gid in safe_ids):\n",
    "                rgi_id = safe.RGI_ID[safe.WGMS_ID == gid].values[0]\n",
    "                lec_id = safe.Leclercq_ID[safe.WGMS_ID == gid].values[0]\n",
    "                gla_id = safe.GlaThiDa_ID[safe.WGMS_ID == gid].values[0]\n",
    "\n",
    "                # RGI x/y\n",
    "                rgi_poly = pdrgi[pdrgi.RGIId == rgi_id].iloc[0]\n",
    "                if rgi_poly.geometry.type == 'Polygon':\n",
    "                    rgi_x, rgi_y = rgi_poly.geometry.exterior.xy\n",
    "                elif rgi_poly.geometry.type == 'MultiPolygon':\n",
    "                    # buffer is necessary as some multi-polygons are self-intersecting\n",
    "                    allparts = [p.buffer(0) for p in rgi_poly.geometry] \n",
    "                    rgi_poly.geometry = shapely.ops.cascaded_union(allparts)\n",
    "                    rgi_x, rgi_y = rgi_poly.geometry.exterior.xy\n",
    "\n",
    "                # GlaThiDa x/y\n",
    "                if not pd.isnull(gla_id):\n",
    "                    print gla_id\n",
    "                    gla_x, gla_y = gla[gla.GlaThiDa_ID == gla_id].LON.values[0], gla[gla.GlaThiDa_ID == gla_id].LAT.values[0]\n",
    "                else:\n",
    "                    gla_x, gla_y = np.nan, np.nan\n",
    "\n",
    "                # Leclercq x/y\n",
    "                if not pd.isnull(lec_id):\n",
    "                    lec_x, lec_y = lec[lec.ID == lec_id].lon.values[0], lec[lec.ID == lec_id].lat.values[0]\n",
    "                else:\n",
    "                    lec_x, lec_y = np.nan, np.nan\n",
    "                \n",
    "                mmlon = [np.nanmin(np.append(np.append(np.append(mmlon, rgi_x), gla_x), lec_x)),\n",
    "                         np.nanmax(np.append(np.append(np.append(mmlon, rgi_x), gla_x), lec_x))]\n",
    "                mmlat = [np.nanmin(np.append(np.append(np.append(mmlat, rgi_y), gla_y), lec_y)),\n",
    "                         np.nanmax(np.append(np.append(np.append(mmlat, rgi_y), gla_y), lec_y))]\n",
    "\n",
    "                # Make a local map where to plot the polygons\n",
    "                local = datasets.GoogleVisibleMap(x=mmlon, y=mmlat) # also possible:  maptype='terrain'\n",
    "                local_map = Map(local.grid, countries=False, nx=640)\n",
    "                local_map.set_lonlat_countours()\n",
    "\n",
    "                # Prepare the figure\n",
    "                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "                ax1.set_title('{}: '.format(gid) + glacier.POLITICAL_UNIT + '-' + glacier.NAME)\n",
    "\n",
    "                # RGI polygon label\n",
    "                if rgi_poly.Name == None or len(rgi_poly.Name)==1:\n",
    "                    plabel =  str(rgi_poly.RGIId)+'\\n'+str(rgi_poly.Area)+'km2'\n",
    "                else:\n",
    "                    plabel =  str(rgi_poly.RGIId)+'\\n'+str(rgi_poly.Area)+'km2\\n'+rgi_poly.Name.encode('ascii', 'ignore')\n",
    "\n",
    "                # RGI centroid\n",
    "                local_map.set_geometry(shpg.Point(rgi_poly.CenLon, rgi_poly.CenLat), edgecolor='k', marker='x', \n",
    "                                       linewidth=4, markersize=100, zorder=50, text='RGI', text_delta=(0.01, 0.0))\n",
    "                local_map.set_geometry(rgi_poly.geometry.exterior, color=colors[i], linewidth=3, label=plabel)\n",
    "\n",
    "                # Plot the WGMS point\n",
    "                local_map.set_geometry(shpg.Point(lon, lat), color='g', marker='x', linewidth=4, markersize=100, \n",
    "                                       zorder=50, text='WGMS', text_delta=(0.01, 0.01))\n",
    "\n",
    "                # Plot the GlaThiDa point\n",
    "                if not pd.isnull(gla_id):\n",
    "                    local_map.set_geometry(shpg.Point(gla_x, gla_y), color='g', marker='x', linewidth=4, markersize=100, \n",
    "                                           zorder=50, text='GLA', text_delta=(0.01, -0.01))\n",
    "                \n",
    "                # Plot the Leclercq point\n",
    "                if not pd.isnull(lec_id):\n",
    "                    local_map.set_geometry(shpg.Point(lec_x, lec_y), color='g', marker='x', linewidth=4, markersize=100, \n",
    "                                           zorder=50, text='LEC', text_delta=(0.01, -0.02))\n",
    "\n",
    "                local_map.set_rgb(local.get_vardata())\n",
    "                local_map.visualize(ax=ax1, addcbar=False)\n",
    "\n",
    "                local = datasets.GoogleVisibleMap(x=mmlon, y=mmlat, maptype='terrain')\n",
    "                local_map.set_rgb(local.get_vardata())\n",
    "                local_map.visualize(ax=ax2, addcbar=False)\n",
    "                plt.subplots_adjust(left=0.04, right=0.80, top=0.94, bottom=0.07)\n",
    "                handles, labels = ax1.get_legend_handles_labels()\n",
    "                dummyline = plt.Line2D((0,1),(0,0), color='w', linewidth=0, linestyle='') # for the 'ALREADY CHECKED' entry\n",
    "                plt.legend(handles+[dummyline], labels+['ALREADY CHECKED'],\n",
    "                           bbox_to_anchor=(1.02, 1.), fontsize=18, loc=2, borderaxespad=0, frameon=False, numpoints=1,\n",
    "                           scatterpoints=1)\n",
    "                pdf.savefig(fig)\n",
    "                plt.close()\n",
    "                \n",
    "                # write results\n",
    "                results.GlaThiDa_ID[results.WGMS_ID == gid] = gla_id\n",
    "                results.Leclercq_ID[results.WGMS_ID == gid] = lec_id\n",
    "                results.RGI_ID[results.WGMS_ID == gid] = rgi_id\n",
    "                results.remark[results.WGMS_ID == gid] = 'ok'\n",
    "                continue\n",
    "            ##########################################################################################################\n",
    "            \n",
    "\n",
    "            # for all other glaciers in the world\n",
    "            for i in np.arange(0,5):\n",
    "                rgig = sortrgi.iloc[i]\n",
    "                # In case the glacier is a MultiPolygon we account for this here:\n",
    "                if rgig.geometry.type == 'Polygon':\n",
    "                    rgi_x, rgi_y = rgig.geometry.exterior.xy\n",
    "                elif rgig.geometry.type == 'MultiPolygon':\n",
    "                    # buffer is necessary as some multi-polygons are self-intersecting\n",
    "                    #allparts = [p.buffer(0) for p in rgig.geometry] \n",
    "                    #rgig.geometry = shapely.ops.cascaded_union(allparts)\n",
    "                    rgig.geometry = rgig.geometry.convex_hull\n",
    "                    rgi_x, rgi_y = rgig.geometry.exterior.xy\n",
    "\n",
    "                mmlon = [np.min(np.append(mmlon, rgi_x)), np.max(np.append(mmlon, rgi_x))]\n",
    "                mmlat = [np.min(np.append(mmlat, rgi_y)), np.max(np.append(mmlat, rgi_y))]\n",
    "\n",
    "            # Make a local map where to plot the polygons\n",
    "            local = datasets.GoogleVisibleMap(x=mmlon, y=mmlat) # also possible:  maptype='terrain'\n",
    "            local_map = Map(local.grid, countries=False, nx=640)\n",
    "            local_map.set_lonlat_countours()\n",
    "            \n",
    "            # find the closest GlaThiDa and Leclercq points like this\n",
    "            gi, gj = local_map.grid.transform(gla.LON.values, gla.LAT.values, nearest=True, maskout=True)\n",
    "            gin = np.where(gi.mask == False)[0] # 'GlaThiDa INside map'\n",
    "            \n",
    "            li, lj = local_map.grid.transform(lec.lon.values, lec.lat.values, nearest=True, maskout=True)\n",
    "            lin = np.where(li.mask == False)[0] # 'Leclercq INside map\n",
    "\n",
    "            # Prepare the figure\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "            ax1.set_title('{}: '.format(gid) + glacier.POLITICAL_UNIT.decode(\"ascii\", \"ignore\") + '-' + \n",
    "                          glacier.NAME.decode(\"ascii\", \"ignore\"))\n",
    "            ax2.set_title(\"Smallest Haversine distance: %.2f m\" % \n",
    "                          (haversine(lon, lat, sortrgi.iloc[0].CenLon, sortrgi.iloc[0].CenLat)))\n",
    "\n",
    "            colors = ['red', 'orange', 'green', 'blue', 'purple', 'magenta']\n",
    "            for i in np.arange(0,5):\n",
    "                rgig = sortrgi.iloc[i]\n",
    "                # In case the glacier is a MultiPolygon we (again) account for this here:\n",
    "                if rgig.geometry.type == 'Polygon':\n",
    "                    rgi_x, rgi_y = rgig.geometry.exterior.xy\n",
    "                elif rgig.geometry.type == 'MultiPolygon':\n",
    "                    # buffer is necessary as some multi-polygons are self-intersecting\n",
    "                    allparts = [p.buffer(0) for p in rgig.geometry] \n",
    "                    rgig.geometry = shapely.ops.cascaded_union(allparts)\n",
    "                    if rgig.geometry.type == 'MultiPolygon':\n",
    "                        print 'ERROR: Polygon %s, %s still a Multipolygon' % (rgig.RGIId, rgig.Name)\n",
    "                        continue\n",
    "                    rgi_x, rgi_y = rgig.geometry.exterior.xy\n",
    "                    #rgig.geometry = rgig.geometry.convex_hull\n",
    "                    #rgi_x, rgi_y = rgig.geometry.exterior.xy\n",
    "\n",
    "                #  print centroid of matching glacier\n",
    "                if i == 0:\n",
    "                    local_map.set_geometry(shpg.Point(rgig.CenLon, rgig.CenLat), edgecolor='k', marker='x', linewidth=4, \n",
    "                                           markersize=100, zorder=50, text='RGI', text_delta=(0.01, 0.0))\n",
    "\n",
    "                # RGI polygon label\n",
    "                if not rgig.Name or len(rgig.Name)==1:  # sometimes there is only one strange letter given\n",
    "                    plabel =  str(rgig.RGIId)+'\\n'+str(rgig.Area)+'km2'\n",
    "                else:\n",
    "                    #try:\n",
    "                    plabel =  str(rgig.RGIId)+'\\n'+str(rgig.Area)+'km2\\n'+rgig.Name.encode('ascii', 'ignore')\n",
    "                    #except UnicodeEncodeError:\n",
    "                    #    plabel =  str(rgig.RGIId)+'\\n'+str(rgig.Area)+'km2\\n'+rgig.Name.encode('ascii', 'ignore')\n",
    "\n",
    "                local_map.set_geometry(rgig.geometry.exterior, color=colors[i], linewidth=3, label=plabel)\n",
    "                try:\n",
    "                    local_map.set_geometry(shpg.Point(lec[lec.index.isin(lin)].lon.values[i], \n",
    "                                                      lec[lec.index.isin(lin)].lat.values[i]), \n",
    "                                           color=colors[i], marker='x', linewidth=4, markersize=100, zorder=50-i, \n",
    "                                           text='LEC:'+str(lec[lec.index.isin(lin)].name.values[i])+\n",
    "                                           str(lec[lec.index.isin(lin)].ID.values[i]), text_delta=(0.01, 0.01))\n",
    "                except IndexError:  # when no equivalent equists or <5 equivalents on grid\n",
    "                    pass\n",
    "                try:\n",
    "                    local_map.set_geometry(shpg.Point(gla[gla.index.isin(gin)].LON.values[i], \n",
    "                                                      gla[gla.index.isin(gin)].LAT.values[i]), \n",
    "                                           color=colors[i], marker='x', linewidth=4, markersize=100, zorder=50-i,\n",
    "                                           text='GLA:'+str(gla[gla.index.isin(gin)].GLACIER_NAME.values[i]).encode('ascii', 'ignore')+\n",
    "                                       str(gla[gla.index.isin(gin)].GlaThiDa_ID.values[i]).encode('ascii', 'ignore'), text_delta=(0.01, -0.01))\n",
    "                except IndexError:  # when no equivalent equists or <5 equivalents on grid\n",
    "                    pass\n",
    "                except UnicodeEncodeError:\n",
    "                    print 'UniCodeEncodeError'\n",
    "                    pass\n",
    "\n",
    "\n",
    "            local_map.set_geometry(shpg.Point(rgig.CenLon, rgig.CenLat), c='k', marker='x', markersize=30, zorder=51) \n",
    "            # Plot the WGMS point\n",
    "            local_map.set_geometry(shpg.Point(lon, lat), color='g', marker='x', linewidth=4, markersize=100, zorder=50, \n",
    "                                   text='WGMS', text_delta=(0.01, -0.02))\n",
    "\n",
    "            local_map.set_rgb((local.get_vardata())[..., 0:3])\n",
    "            local_map.visualize(ax=ax1, addcbar=False)\n",
    "\n",
    "            local = datasets.GoogleVisibleMap(x=mmlon, y=mmlat, maptype='terrain')\n",
    "            local_map.set_rgb((local.get_vardata())[..., 0:3])\n",
    "            local_map.visualize(ax=ax2, addcbar=False)\n",
    "            plt.subplots_adjust(left=0.04, right=0.80, top=0.94, bottom=0.07)\n",
    "            plt.legend(bbox_to_anchor=(1.02, 1.), fontsize=18, loc=2, borderaxespad=0, frameon=False, numpoints=1, scatterpoints=1)\n",
    "            \n",
    "            pdf.savefig(fig)\n",
    "            plt.close()\n",
    "            \n",
    "            # write results in one step\n",
    "            results.RGI_ID[results.WGMS_ID == gid] = sortrgi.iloc[0].RGIId\n",
    "            gla['DIST'] = haversine(lon, lat, gla.LON.values, gla.LAT.values)\n",
    "            sortgla = gla.sort(columns='DIST')\n",
    "            results.GlaThiDa_ID[results.WGMS_ID == gid] = sortgla.iloc[0].GlaThiDa_ID\n",
    "            lec['DIST'] = haversine(lon, lat, lec.lon.values, lec.lat.values)\n",
    "            sortlec = lec.sort(columns='DIST')\n",
    "            results.Leclercq_ID[results.WGMS_ID == gid] = sortlec.iloc[0].ID\n",
    "            \n",
    "\n",
    "            if curr % 5 == 0:\n",
    "                print \"%s / %s plots done.\" % (curr, total)\n",
    "            curr += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results.to_csv(os.path.join(output_dir, 'Automated_links_WGMS_to_RGI_GlaThiDa_Leclercq.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
